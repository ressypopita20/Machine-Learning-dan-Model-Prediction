# -*- coding: utf-8 -*-
"""Laporan Praktikum Machine Learning

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QXAXZmRsJrmwqgDieEGrh07apVcAXmwS

# **Laporan Praktikum I Machine Learning**

'''
=================================================
                      Oleh:
Nama  : Resi Popita

NPM   : F1F022036

# **Ini adalah program computer vision untuk mengklasifikasi jenis Patah Tulang dari hasil rontgen pasien menggunakan Deep Learning.**
=================================================
'''

# Latar Belakang

Patah tulang (fracture) merupakan kondisi klinis yang penting dan memerlukan diagnosis yang cepat dan akurat untuk mencegah komplikasi jangka panjang. Radiografi (rontgen) adalah metode utama dalam mendiagnosis patah tulang karena sifatnya yang mudah diakses dan relatif murah. Namun, interpretasi manual citra rontgen sangat bergantung pada keahlian radiolog atau dokter ortopedi, sehingga terdapat potensi kesalahan serta variabilitas antar pengamat (Su Z et al, 2023)

Di sisi lain, kemajuan dalam deep learning dan computer vision membuka peluang besar untuk otomasi diagnosis patah tulang. Beberapa penelitian telah menunjukkan bahwa model Convolutional Neural Network (CNN) dan transfer learning dapat mencapai akurasi tinggi. Contohnya, studi Novel Transfer Learning Based Bone Fracture Detection melaporkan bahwa menggunakan jaringan pre-trained dapat meningkatkan deteksi patah tulang dari citra radiografi (Alam S et al., 2025). Penelitian lain menggunakan ensemble model (gabungan beberapa CNN seperti MobileNetV2, VGG16, InceptionV3, ResNet50) dan preprocessing seperti histogram equalization, memberikan performa hingga ~92,96% akurasi dalam mendeteksi patah tulang humerus (Aldhyani T et al., 2025). Selain itu, metode CNN yang dikombinasikan dengan algoritma genetik juga telah diterapkan untuk mendeteksi patah tulang leher femur, menunjukkan potensi meskipun dataset terbatas (Beyas S et al., 2020).

Tinjauan sistematis juga menunjukkan bahwa deteksi patah tulang lewat deep learning mencakup berbagai tugas: pengenalan (recognition), klasifikasi jenis patah, deteksi posisi patahan, hingga lokalisasi menggunakan heatmap atau bounding boxes (Tanzi L et al., 2020). Hal ini memperkuat pentingnya desain sistem yang tidak hanya akurat tetapi juga dapat menjelaskan keputusan diagnostik secara medis.

Berdasarkan latar ini, dalam pengembangan sistem klasifikasi patah tulang dari citra rontgen, sangat relevan untuk menerapkan strategi feature engineering yang menggabungkan fitur hand-crafted (misalnya tepi, tekstur) dan fitur yang dipelajari melalui deep learning. Hal ini tidak hanya dapat meningkatkan akurasi, tetapi juga membuat sistem lebih interpretatif dan layak digunakan dalam praktik klinis

# Rumusan Masalah

Adapun rumusan masalah pada praktikum ini adalah sebagai berikut:
1. Bagaimana mahasiswa dapat memahami konsep dari berbagai jenis *feature engineering* pada Python
2. Bagaimana mahasiswa dapat melakukan teknik *feature engineering* di program Python

# Tujuan Penelitian

Berdasarkan rumusan masalah diatas diperoleh tujuan sebagai berikut:


1.   Mahasiswa dapat memahami konsep dari berbagai jenis *feature engineering* pada Python
2.   Mahasiswa dapat melakukan teknik  *feature engineering* di program Python

# Mengimpor _Libraries_

Berikut merupakan _library_ yang akan digunakan dalam _notebook_ ini:
"""

!pip install feature_engine

# Commented out IPython magic to ensure Python compatibility.
# Define Libraries

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
import warnings
warnings.filterwarnings('ignore')

# Split Dataset and Standarize the Datasets
from sklearn.model_selection import train_test_split

# Scaling
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler

#Encoding
from sklearn.preprocessing import OneHotEncoder

#Handling Outlier
from feature_engine.outliers import Winsorizer

#Balancing Data
from imblearn.over_sampling import SMOTE

#model ML
from sklearn.linear_model import LogisticRegression

#model performence
from sklearn.metrics import classification_report, f1_score

from sklearn.impute import SimpleImputer ##Imputasi Simple berdasarkan modus
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer## Imputasi MICE

"""# Memuat Data

Sebelum kita membahas lebih jauh tentang Feature Engineering, berikut ini adalah beberapa dataset yang akan digunakan untuk memperjelas tujuan dari notebook ini.
"""

# Memasukkan dataset

df_ori = pd.read_csv('https://raw.githubusercontent.com/Royallist/DATA-MACHINE-LEARNING/refs/heads/main/data_penelitian.csv', sep=';')
data_pt1 = df_ori.copy()
data_pt1.head(10)

# Melihat dimensi data
data_pt1.shape

#melihat tipe data
data_pt1.dtypes

"""# Analisis Data Eksploratif

# _Feature Engineering_

## A - Missing Value

Data hilang (***missing data***) atau nilai hilang (***missing values***) terjadi ketika tidak ada data atau nilai yang tersimpan untuk suatu observasi pada variabel tertentu.

Kehilangan data adalah hal yang sering terjadi, baik dalam kompetisi data science maupun di dunia bisnis, dan dapat mempengaruhi kesimpulan yang diambil dari data tersebut. **Data yang tidak lengkap merupakan masalah yang tidak bisa dihindari saat bekerja dengan berbagai sumber data.**

---
### - Kenapa data hilang (data missing)?

Ada berbagai alasan mengapa data bisa hilang, antara lain:
* Nilai bisa **hilang** atau **tidak tersimpan dengan benar** saat proses pengumpulan data.
  
  Misalnya, data berasal dari survei yang diisi secara manual ke dalam formulir online. Petugas entri data bisa saja lupa mengisi satu kolom dalam formulir tersebut, sehingga nilai pada kolom itu menjadi hilang.

* **Nilai memang tidak ada.**

  Contohnya: untuk variabel yang dihitung dari hasil pembagian antara dua variabel, seperti rasio utang terhadap pendapatan. Jika seseorang tidak memiliki pendapatan, maka nilai rasio tersebut tidak dapat dihitung karena pembagian dengan nol tidak terdefinisi.


* Data hilang juga bisa terjadi karena **responden menolak menjawab** pertanyaan tertentu dalam formulir.
  
  Misalnya, seseorang mungkin enggan mengungkapkan jumlah pendapatannya. Akibatnya, nilai untuk variabel “pendapatan” akan menjadi hilang bagi orang tersebut.

Selain memahami penyebab hilangnya data, **penting juga untuk memahami mekanisme bagaimana nilai-nilai yang hilang itu muncul dalam dataset**.
Tergantung pada mekanismenya, kita bisa memilih untuk memproses nilai yang hilang dengan cara yang berbeda. Selain itu, dengan mengetahui sumber data yang hilang, kita juga bisa mengambil langkah untuk mengendalikan sumber tersebut, dan mengurangi jumlah data yang hilang di masa depan selama proses pengumpulan data.
"""

# Mengecek data yang hilang

data_pt1.isnull().sum()

# Periksa nilai yang hilang sebagai persentase terhadap total data

data_pt1.isnull().mean()

#Missing value kolom Keadaan Cuaca
data_pt1[data_pt1['Keadaan Cuaca'].isnull()].head(10)

"""### Penanganan Data Missing

#### -Drop Missing Data
"""

#Drop data hilang
data_pt1_dropna=data_pt1.dropna()
data_pt1_dropna.isnull().sum()

data_pt1_dropna.shape

"""#### -Imputasi Missing Data"""

#Imputasi Maju
# 'ffill' = forward fill
df_imp1=data_pt1.copy()
df_imp1['Keadaan Cuaca'] = df_imp1['Keadaan Cuaca'].fillna(method='ffill')
#'bfill' = backward fill
#df_imp1['Keadaan Cuaca'] = df_imp1['Keadaan Cuaca'].fillna(method='bfill')
df_imp1.isnull().sum()

df_imp1.iloc[41]

df_imp2=data_pt1.copy()
# Hitung  mean
mean_value = df_imp2['Kecepatan Angin'].mean()

# Isi NaN dengan mean
df_imp2['Kecepatan Angin'] = df_imp2['Kecepatan Angin'].fillna(mean_value)
df_imp2.isnull().sum()

df_imp3=data_pt1.copy()
# Hitung median
median_value = df_imp3['Kecepatan Angin'].median()

# Isi NaN dengan median
df_imp3['Kecepatan Angin'] = df_imp3['Kecepatan Angin'].fillna(median_value)
df_imp3.isnull().sum()

df_imp4=data_pt1.copy()

# Isi NaN dengan interpolasi
df_imp4['Keadaan Cuaca'] = df_imp4['Keadaan Cuaca'].interpolate(method='linear')
df_imp4.isnull().sum()

median_value= df_imp4['Kecepatan Angin'].median()
# Isi NaN dengan median
df_imp4['Kecepatan Angin'] = df_imp4['Kecepatan Angin'].fillna(median_value)
df_imp4.isnull().sum()

df_imp5=data_pt1.copy()

imp = IterativeImputer(random_state=42)
df_imp5[['Kecepatan Angin']] = imp.fit_transform(df_imp5[['Kecepatan Angin']])

mode_imputer = SimpleImputer(strategy='most_frequent')
df_imp5[['Keadaan Cuaca']] = mode_imputer.fit_transform(df_imp5[['Keadaan Cuaca']])

# Cek hasil
df_imp5.isnull().sum()

"""## B - Kardinalitas"""

df_imp5.head(10)

df_imp5['Keadaan Cuaca'].unique()

# Mengurangi Kardinalitas

bins = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]
# Tentukan Label untuk Setiap Bin
# 10 label (0 s/d 9)
labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]

# Terapkan pd.cut
df_imp5['Keadaan_Cuaca_reduced'] = pd.cut(
    df_imp4['Keadaan Cuaca'],
    bins=bins,
    labels=labels,
    right=True,
    include_lowest=True
)

# Verifikasi
print("--- Hasil Perbandingan ---")
# Menampilkan 10 data acak
print(df_imp5.sample(10))

print("\n--- Pengecekan Kardinalitas ---")
print('Jumlah kategori di "Keadaan Cuaca" asli : {}'.format(len(df_imp5['Keadaan Cuaca'].unique())))
print('Jumlah kategori di "Keadaan_Cuaca_reduced" : {}'.format(len(df_imp5['Keadaan_Cuaca_reduced'].unique())))

print("\nKategori unik yang baru (reduced):")
print(df_imp5['Keadaan_Cuaca_reduced'].unique())

"""## C- Splitting Data"""

# menjelaskan variabel x dan variabel y
df_imp5=df_imp5.drop('Keadaan Cuaca', axis = 1)
X= df_imp5.drop('Hujan', axis = 1)
y=df_imp4['Hujan']

X_train,X_test,y_train,y_test =train_test_split(X,y,test_size=0.2,random_state=200, shuffle= False)

#X_train,X_test,y_train,y_test =train_test_split(X,y,test_size=0.2,random_state=200, shuffle= True)
#X_train.shape

#X_train,X_test,y_train,y_test =train_test_split(X,y,test_size=0.2,random_state=200, stratify=y)
#X_test.shape

"""## D- Handling Outlier"""

# handling dengan fitur numerik interquartile range
list_num = ['Suhu Udara', 'Kelembapan', 'Kecepatan Angin']
# Hitung IQR
list_outlier=[]
list_lower_bound =[]
list_upper_bound =[]
for i in X_train[list_num]:
    Q1 = np.percentile(X_train[i], 25)
    Q3 = np.percentile(X_train[i], 75)
    IQR = Q3 - Q1

    # Tentukan batas bawah dan batas atas
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    list_lower_bound.append(lower_bound)
    list_upper_bound.append(upper_bound)

    # Menambahkan lower_bown dan upper_bown
    num_outliers_lower = np.sum(X_train[i] < lower_bound)
    num_outliers_upper = np.sum(X_train[i] > upper_bound)

    # Jumlahkan outlier
    total_outliers = num_outliers_lower + num_outliers_upper
    list_outlier.append(total_outliers)

# Mendefinikan dataframe outlier yang baru
outliers = pd.DataFrame()
outliers['Kolom'] = list_num
outliers['Jumlah Outlier'] = list_outlier
outliers['Lower Bound'] = list_lower_bound
outliers['Upper Bound'] = list_upper_bound
outliers

"""Dari hasil analisa diketahui bahwa 1 kolom fitur memiliki outlier pada data dengan jumlah yang besar."""

# Mendefinisikan kolom nilai normal dan tidak
df_num = X_train[list_num]
nilai_skew = []
nilai_skew_normal = []

# Membuat fungsi untuk memetakan distribusi setiap kolom
for i in df_num.columns:
    skewness = X_train[i].skew()
    if -0.5 <= skewness <= 0.5:
        nilai_skew_normal.append(i)
    else:
        nilai_skew.append(i)

# Hasil
print('kolom yang mempunyai nilai skewness sedang:', nilai_skew)
print('kolom yang mempunyai nilai skewness normal:', nilai_skew_normal)

"""Setelah dilihat sebaran datanya, diketahui bahwa kolom fitur kecepatan angin berdistribusi skew

#### Trimming
"""

outliers_indexed = outliers.set_index('Kolom')
# Ambil nilai lower dan upper berdasarkan indeks 'Kecepatan Angin'
lower_kecepatan = outliers_indexed.loc['Kecepatan Angin', 'Lower Bound']
upper_kecepatan = outliers_indexed.loc['Kecepatan Angin', 'Upper Bound']

# Flag the outliers in category
outliers_KecepAngin = np.where(X_train['Kecepatan Angin'] > upper_kecepatan, True,
                       np.where(X_train['Kecepatan Angin'] < lower_kecepatan, True, False))


# Trimming the dataset
X_train_trimmed1 = X_train.loc[~(outliers_KecepAngin)]

print('Size dataset - Before trimming : ', X_train.shape)
print('Size dataset - After trimming  : ', X_train_trimmed1.shape)

"""#### Capping"""

# Capping the skew distribution

winsoriser_Kecepatan_Angin = Winsorizer(capping_method='iqr', #gaussian for normal distribution
                            tail='both',
                            fold=1.5,
                            variables=['Kecepatan Angin'],
                            missing_values='ignore')

X_train_capped = winsoriser_Kecepatan_Angin.fit_transform(X_train)
X_test_capped = winsoriser_Kecepatan_Angin.transform(X_test)

"""#### visualisasi"""

# Defining a function to create histogram and box plot for certain variables
def diagnostic_plots(df, variable):
    # Define figure size
    plt.figure(figsize=(10, 6))

    # Histogram
    plt.subplot(1, 2, 1)
    sns.histplot(df[variable], bins=30, color='teal')
    plt.title('Histogram')

    # Boxplot
    plt.subplot(1, 2, 2)
    sns.boxplot(y=df[variable], color='teal')
    plt.title('Boxplot')

    plt.show()

# Plotting histogram and boxplot before and after capping
for col in list_num:
  print(f'{col} - Before Capping')
  diagnostic_plots(X_train, col)

  print(f'\n{col} - After Capping')
  diagnostic_plots(X_train_capped, col)

"""## E- Scaling"""

# Mendefinisikan kolom yang digunakan dan list tempat value skewness
nilai_skew = []
nilai_skew_normal = []

# Membuat fungsi untuk memetakan distribusi setiap kolom
for i in df_num.columns:
    skewness = X_train_capped[i].skew()
    if -0.5 <= skewness <= 0.5:
        nilai_skew_normal.append(i)
    else:
        nilai_skew.append(i)

# Hasil
print('kolom yang mempunyai nilai skewness sedang:', nilai_skew)
print('kolom yang mempunyai nilai skewness normal:', nilai_skew_normal)

# Melakukan scalling menggunakan standard scaller
standard_scaler = StandardScaler()
standard_scaler.fit(X_train_capped[nilai_skew_normal])
X_train_capped[nilai_skew_normal] = standard_scaler.transform(X_train_capped[nilai_skew_normal])
X_test_capped[nilai_skew_normal] = standard_scaler.transform(X_test_capped[nilai_skew_normal])

# Melakukan scalling menggunakan robust scaller
robust_scaler = RobustScaler()
robust_scaler.fit(X_train_capped[nilai_skew])
X_train_capped[nilai_skew] = robust_scaler.transform(X_train_capped[nilai_skew])
X_test_capped[nilai_skew] = robust_scaler.transform(X_test_capped[nilai_skew])

X_train_scale = X_train_capped
X_test_scale = X_test_capped

"""## F- Encoding

### One Hot Encoder
"""

list_cat = ['Keadaan_Cuaca_reduced']

encoder = OneHotEncoder()

# Melakukan encoding
X_train_encoded = encoder.fit_transform(X_train_scale[list_cat])
X_test_encoded = encoder.transform(X_test_scale[list_cat])

"""### Ordinal Encoder"""

# lis_ord = ['smoker']
# smoker_cat = ['no', 'yes'] #mengatur urutan
# ord_enc = OrdinalEncoder(categories= [smoker_cat]) #mendefinisikan fungsi encoding

# #melakukan encoding
# X_train_select[lis_ord] = ord_enc.fit_transform(X_train_select[['smoker']])
# X_test_select[lis_ord] = ord_enc.transform(X_test[['smoker']])

"""## G- Balancing Data"""

# Balancing data
y_train.value_counts()

# Oversampling with SMOTE

smote = SMOTE(sampling_strategy='minority', k_neighbors=5, random_state=42)     # Minority Class will have same the same number as the majority class
# smote = SMOTE(sampling_strategy={1:1000}, k_neighbors=5, random_state=42)     # Minority Class will have 1000 data
X_train_balanced, y_train_balanced = smote.fit_resample(X_train_encoded, y_train)
y_train_balanced.value_counts()

"""# _Training Model_"""

# Tanpa feature engineering
lr_1 = LogisticRegression()
lr_1.fit(X_train, y_train)

# Dengan feature engineering
lr_2 = LogisticRegression()
lr_2.fit(X_train_encoded, y_train)

# Dengan feature engineering dan balancing data
lr_3 = LogisticRegression()
lr_3.fit(X_train_balanced, y_train_balanced)



"""# _Model Performance_"""

# HASIL - without Handling

print(classification_report(y_train, lr_1.predict(X_train)))
print(classification_report(y_test, lr_1.predict(X_test)))

# HASIL - with Handling

print(classification_report(y_train, lr_2.predict(X_train_encoded)))
print(classification_report(y_test, lr_2.predict(X_test_encoded)))

# HASIL - with Handling

print(classification_report(y_train_balanced, lr_3.predict(X_train_balanced)))
print(classification_report(y_test, lr_3.predict(X_test_encoded)))

"""# Kesimpulan

Pada praktikum pertemuan 1, dilakukan imputasi untuk mengatasi missing value dan meningkatkan akurasi model. Variabel Kecepatan Angin (numerik) diimputasi menggunakan *IterativeImputer* (MICE), yang memprediksi nilai hilang berdasarkan hubungan antarvariabel secara iteratif. Metode ini lebih akurat dibanding pengisian dengan mean atau median.

Untuk variabel Keadaan Cuaca (kategorik), digunakan SimpleImputer dengan strategi most frequent, yaitu mengganti nilai kosong dengan kategori yang paling sering muncul agar distribusi data tetap konsisten. Hasilnya, seluruh nilai hilang berhasil diisi dan dataset siap digunakan untuk analisis.

Selanjutnya, data dibagi menggunakan fungsi train_test_split dengan parameter test_size=0.2, random_state=200, dan shuffle=False. Sebanyak 80% data digunakan untuk pelatihan dan 20% untuk pengujian, nilai random_state=200 digunakan untuk membagi hasil data dengan konsisten setiap kali di jalankan. Parameter shuffle=False diterapkan karena data bersifat time series, sehingga urutan waktu tetap terjaga dan mencegah data leakage.

# Referensi

## Daftar Pustaka

* Su, Z., Adam, A., Nasrudin, M. F., Ayob, M., & Punganan, G. (2023). *Skeletal Fracture Detection with Deep Learning: A Comprehensive Review*. *Diagnostics, 13*(20), 3245. [https://doi.org/10.3390/diagnostics13203245](https://doi.org/10.3390/diagnostics13203245)
* Alam, A., Al-Shamayleh, A. S., Thalji, N., Raza, A., Morales Barajas, E. A., Bautista Thompson, E., & Ashraf, I. (2025). *Novel transfer learning based bone fracture detection using radiographic images*. *BMC Medical Imaging, 25*, Article 5. [https://doi.org/10.1186/s12880-024-01546-4](https://doi.org/10.1186/s12880-024-01546-4)
* Aldhyani, T., Ahmed, Z. A. T., Alsharbi, B. M., Ahmad, S., Al-Adhaileh, M. H., Kamal, A. H., Almaiah, M., & Nazeer, J. (2025). *Diagnosis and detection of bone fracture in radiographic images using deep learning approaches*. *Frontiers in Medicine*, 11. [https://doi.org/10.3389/fmed.2024.1506686](https://doi.org/10.3389/fmed.2024.1506686)
* Beyaz, S., Açıcı, K., & Sümer, E. (2020). *Femoral neck fracture detection in X-ray images using deep learning and genetic algorithm approaches*. 31(2). *Joint Diseases and Related Surgery*, 175–183. [https://doi.org/10.5606/ehc.2020.72163](https://doi.org/10.5606/ehc.2020.72163)
"""